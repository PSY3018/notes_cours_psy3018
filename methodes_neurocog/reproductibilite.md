---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
(reproductibilite-controverses-chapitre)=
# Reproductibilit√© et controverses
<table>
  <tr>
    <td align="center">
      <a href="https://github.com/elisabethloranger">
        <img src="https://avatars.githubusercontent.com/u/90270981?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>√âlisabeth Loranger</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
    <td align="center">
      <a href="https://github.com/pbellec">
        <img src="https://avatars.githubusercontent.com/u/1670887?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>Pierre bellec</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
  </tr>
</table>

Durant ce cours, on a pass√© en revue diverses techniques de neuroimagerie qui ouvrent une fen√™tre fascinante sur la structure et la fonction du cerveau. Mais ces techniques sont r√©guli√®rement impliqu√©es dans des articles scientifiques qui semblent peu cr√©dibles. Dans ce cours nous allons discuter des controverses autour de la neuroimagerie, et plus g√©n√©ralement de la crise de reproducibilit√© en sciences.

Les objectifs de ce cours sont les suivants :
- Comprendre la crise de reproductibilit√© en sciences.
- Comprendre certaines pratiques scientifiques douteuses qui participent
au manque de reproductibilit√© en neurosciences cognitives.
- Conna√Ætre certains outils qui peuvent am√©liorer la reproductibilit√© en neurosciences cognitives.

## La crise de reproductibilit√©

### Une crise? Quelle crise?
```{figure} ./reproductibilite/significant.png
---
width: 600px
name: significant-fig
---
Cette figure illustre le processus qui am√®ne √† un r√©sultat scientifique controvers√© (et le probl√®me de comparaisons multiples). Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/882/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
En 2016, un sondage aupr√®s de 1576 chercheurs a √©t√© men√© dans le but de voir si, dans la
perception des professionnels dans la recherche, il y a une crise de
reproductibilit√© et si oui, laquelle ([Baker, 2016](https://www.nature.com/articles/533452a#change-history)). En tout, 90% des chercheurs dans ce sondage pensent
qu‚Äôil y a effectivement une crise de reproductibilit√© (52% pour une crise significative et 38% pour une crise mod√©r√©e).

La reproductibilit√©, c‚Äôest quoi ? Si on avait acc√®s
aux donn√©es derri√®re ce papier, est-ce qu‚Äôon serait capable de refaire les
analyses et arriver aux m√™mes conclusions ? Un autre concept proche est la r√©plication: en recrutant de nouveaux sujets et en faisant exactement ce que les autres chercheurs ont fait au niveau des outils utilis√©s et des analyses effectu√©es, est-ce qu'on va trouver les m√™mes r√©sultats ? Dans le sondage, 70% des personnes sond√©es rapportent avoir √©chou√© √† reproduire les r√©sultats d'une autre √©quipe de recherche, et plus de 50% rapportent avoir √©chou√© √† reproduire leurs propres r√©sultats.

Les personnes sond√©es ont aussi √©valu√© les causes probables de cette crise de
reproductibilit√©. Parmi les raisons les plus fr√©quemment mentionn√©es,
on retrouve la _pression √† publier_ et la _publication s√©lective_ (les gens publient
seulement ce qui fonctionne bien) ainsi que la _puissance statistique limit√©e_.
Ce chapitre va expliquer certaines de ces notions plus en d√©tails, en d√©marrant par formaliser le processus de g√©n√©ration de connaissances scientifiques.

### La m√©thode scientifique
```{figure} ./reproductibilite/researchcycle_original.png
---
width: 800px
name: researchcycle-original-fig
---
Cette figure illustre le cycle des d√©couvertes scientifiques, selon l'approche de la m√©thode scientifique d√©crite par [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences). Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure adapt√©e par P. Bellec est elle-m√™me disponible sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```
La figure {numref}`researchcycle-original` pr√©sente une version simplifi√©e de la m√©thode scientifique pour la d√©couverte de connaissances, inspir√©e par la th√©orie de [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences), telle qu'elle est g√©n√©ralement impl√©ment√©e dans la communaut√© de recherche.
 * On commence avec les publications, qui repr√©sentent les connaissances qui ont √©t√© accumul√©es par d‚Äôautres.
 * En lisant cette lit√©rature, les chercheuses/chercheurs peuvent apprendre ce qui a d√©j√† √©t√© d√©couvert, et faire des hypoth√®ses sur des choses qu‚Äôon ne connait pas encore.
 * Les chercheuses/chercheurs vont alors formuler un devis de recherche : nombre de participants, groupes, tests statistiques, etc. Elles/ils vont aussi faire des pr√©dicitions concernant les r√©sultats qu‚Äôelles/ils pensent obtenir.
 * Une fois le devis de recherche √©labor√©, il est temps de recueillir les donn√©es.
 * Ensuite, on analyse les donn√©es en suivant le protocole qui avait √©t√© √©tabli dans le devis de recherche.
 * Il faut alors interpr√©ter les r√©sultats, et notamment les comparer √† nos pr√©dictions pour valider ou invalider nos hypoth√®ses.
 * Les r√©sultats de la recherche sont alors publi√©s pour permettre au reste de la communaut√© de recherche de continuer √† formuler de nouvelles hypoth√®ses.

 Comme on utilise des statistiques rigoureuses dans cette approche, on ne g√©n√®re qu'une quantit√© limit√© de faux positifs, et donc on fait des d√©couvertes scientifiques sans faire trop d'erreurs. En pratique, cette approche peut √™tre adapt√©e de nombreuses mani√®res avec _des pratiques de recherche douteuses_ qui vont compromettre l'int√©grit√© et la rigueur des conclusions de l'√©tude.

### La m√©thode scientifique: hacked
```{figure} ./reproductibilite/researchcycle_hacked.png
---
width: 800px
name: researchcycle-hacked-fig
---
Cette figure illustre les pratiques douteuses qui peuvent affecter n√©gativement l'int√©grit√© du cycle des d√©couvertes scientifiques. Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure int√®gre aussi une image [shutterstock](https://www.shutterstock.com/image-vector/computer-hacker-laptop-icon-787273936), utilis√©e sous licence shutterstock standard.
```
#### Biais de publication
La publication s√©lective est un de probl√®mes les plus importants identifi√©s dans le sondage vu plus t√¥t. Cela signifie que les r√©sultats d'une √©tude ne sont publi√©s que lorsqu‚Äôils ne sont positifs, c'est √† dire uniquement s'ils confirment les hypoth√®ses de l'√©quipe de recherche. Si ce type de pratique est syst√©matique dans une communaut√© de recherche, il se peut que plusieurs groupes rapporte un r√©sultat, qui semble alors  robuste, alors qu'en fait un nombre plus important de groupes de recherche n'ont pas pu r√©pliquer cet effet, mais sans publier. Cela vient d√©former compl√®tement les connaissances accumul√©es par la communaut√© scientifique, qui est √† l'origine des hypoth√®ses des √©tudes futures.

##### p-hacking
Si on voit que nos r√©sultats ne correspondent pas √† nos attentes, on pourrait se demander si on n'a pas commis une erreur ou peut √™tre qu'on n'a pas choisi la technique d'analyse la plus optimale. On va alors revisiter la mani√®re
dont analyse les donn√©es jusqu'√† ce que les r√©sultats deviennent significatifs. Ce type d'approche a √©t√© baptis√© _p-hacking_. Le p-hacking peut prendre de nombreuses formes: exclusion arbitraire de "valeurs aberrantes", s√©lection d'un sous-groupe qui montre l'effet attendu, changement des param√®tres de pr√©traitements.

#### HARKing
La derni√®re pratique douteuse est baptis√©e le ¬´ HARKing ¬ª. Le terme HARK est un acronyme en anglais pour les termes ¬´ Hypothesis after results are known ¬ª, ou bien "d√©finition des hypoth√®ses apr√®s que les r√©sultats soient connus". On va effectuer de nombreux tests √† partir des donn√©es recueillies, et on formule a posteriori des hypoth√®ses correspondant aux r√©sultats significatifs dans l'√©chantillon. Ce processus n'est pas n√©cessairement malicieux, mais peut √©merger d'une volont√© d'interpr√©ter les donn√©es. Cette d√©marche n'est pas n√©cessairement probl√©matique, du moment que les hypoth√®ses sont (correctement) pr√©sent√©es comme exploratoires, guid√©es par les donn√©es, plut√¥t que comme une hypoth√®se a priori rigoureuse.

Nous allons maintenant voir comment la neuroimagerie repr√©sente un domaine particuli√®rement propice au p-hacking, et d'autres facteurs qui contribuent au manque de reproductibilit√©. 

## Reproducibilit√© et neuroimagerie
On va maintenant voir quels probl√®mes s‚Äôappliquent directement √† la
neuroimagerie et comment certains probl√®mes de reproductibilit√©
sp√©cifiques peuvent ressortir lors de l‚Äôutilisation de logiciels en
neuroimagerie.

### Degr√©s de libert√© en recherche
Le premier probl√®me qui est exacerb√© en neuroimagerie est le nombre
d‚ÄôAnalyses qu‚Äôil faut faire ainsi que le nombre de choix qu‚Äôil faut faire en lien
avec ces analyses. Ceci est reli√© au probl√®me de p-hacking dont on a discut√©
plus t√¥t. Ainsi, toutes les possibilit√©s d‚Äôanalyses permettent aux chercheurs
d‚Äôaller faire ressortir l‚Äôeffet qu‚Äôils d√©sirent observer d‚Äôune fa√ßon ou d‚Äôune
autre. Cette vaste s√©lection d‚Äôanalyses est le r√©sultat de dizaines de
param√®tres modifiables qui sont impliqu√©s dans ces analyses. Une √©tude a
tent√© de syst√©matiquement changer des choix de param√®tres d‚Äôanalyse pour
savoir quels r√©sultats ils pouvaient obtenir. En bout de ligne, ils ont obtenu
pr√®s de 7000 chaines de traitement qui pouvaient √™tre ex√©cut√©es. Ensuite,
l‚Äô√©quipe a essay√© diff√©rents seuils statistiques ce qui a g√©n√©r√© pr√®s de 35
000 cartes d‚Äôactivation. Ceci repr√©sente la vari√©t√© des r√©sultats qu‚Äôil est
possible d‚Äôobtenir en utilisant un simple contraste d‚Äôimages IRMf. On parle
aussi de Multiverse, dans le sens ou l‚Äôhistoire statistique peut changer
compl√®tement avec une seule petite modification dans le choix des
param√®tres d‚Äôanalyse. Dans le cas des images d‚ÄôIRM, on a tellement plus de
donn√©es et tellement plus d‚Äôanalyses a faire que le probl√®me de Multiverse
est d‚Äôautant plus prononc√© comparativement aux analyses statistiques
traditionnelles.
On voit sur l‚Äôimage ici que les r√©gions qui pr√©sentent la plus haute moyenne
sont aussi les r√©gions qui ont la plus grande variabilit√© dans les donn√©es,
m√™me que l‚Äôamplitude de la variation est plus grande que l‚Äôamplitude de la
moyenne. Cela sugg√®re qu‚Äôil y a beaucoup de variabilit√© sur les cartes finales
par les diff√©rents choix analytiques choisis. Dans ce cas, les changements de
param√®tres ont √©t√© fait plut√¥t syst√©matiquement et les gens dans la vraie vie
ne jouent pas autant avec les param√®tres.

### Logiciels d'analyse
Maintenant, imaginons qu‚Äôon fait les analyses dans un des multiples logiciels
ci bas. Si les r√©sultats escompt√©s ne sont pas obtenus avec un des logiciels,
on pourrait aller faire les m√™mes analyses avec un autre logiciel et obtenir
des r√©sultats diff√©rents.
Donc √† droite on a les logos des 3 logiciels principaux, AFNI, FSL et SPM. Par
ailleurs, il y a plein de versions de SPM. La premi√®re version utilis√©e de
mani√®re large est la version 95, apr√®s il y a eu 96, 99, SPM8 et SMP 12. En
plus, chaque version de SPM a eu des versions mineures qui ne sont pas ici.
En fait, ce qu‚Äôil faut retenir, c‚Äôest que ce qui a √©t√© expliqu√© en lien avec le
choix des analyses s‚Äôapplique aussi au choix du logiciel d‚Äôanalyse utilis√© en
IRM. M√™me si on fait des choix comparables d‚Äôanalyses dans les diff√©rents
logiciels on va se retrouver avec des variations.
Donc une √©tude de Bowring a tent√© de prendre un jeu de donn√©es public et
des cartes d‚Äôactivation qui ont √©t√© g√©n√©r√©es dans un article et ils ont essay√©
de reproduire la carte d‚ÄôActivation de l‚Äôarticle avec les 3 logiciels suivants :
FSL, AFNI, SPM. Les colonnes qu‚Äôon voit ici c‚Äôest 3 jeux de donn√©es diff√©rents,
en bas on a la carte d‚Äôactivation qui a publi√©e par le groupe dans l‚Äôarticle
original et ce qu‚Äôon a dans chaque ligne au-dessus c‚Äôest ce qui a √©t√© obtenu
avec ces donn√©es √† l‚Äôaide des 3 logiciels. Si on se concentre sur le premier
jeu de donn√©es √† gauche, on voit que finalement on voit des activations au
niveau du cingulaire ant√©rieur et de l‚Äôorbitofrontal ventral et une activation
n√©gative au niveau du cortex cingulaire post√©rieur. Avec FSL, on retrouve le
m√™me genre d‚ÄôActivation mais avec des positions et des amplitudes
diff√©rentes. Des nouvelles activations ressortent au niveau occipital et au
niveau sous cortical qui ne sont pas l√† dans la carte d‚Äô‚Äôactivation publi√©e.
Pour SPM, l‚Äôamplitude et la position des activations sont variables
comparativement a l‚Äôoriginal. Tant au niveau qualitatif que quantitatif, la
coh√©rence de ces 4 cartes d‚Äôactivation est assez faible. C‚Äôest une belle
illustration, on voit qu‚Äôil y a un impact assez important du logiciel qu‚Äôon
utilise pour faire les cartes, ce qui en dit gros sur la reproductibilit√©. M√™me
avec les m√™mes donn√©es, seul le logiciel utilis√© pour les analyses peut avoir
un impact important sur les r√©sultats finaux des analyses. Quand on parlait
de p-hacking et du fait qu‚Äôon a plein d‚Äô√©tapes qui peuvent √™tre modifi√©es, le
logiciel utilis√© s‚Äôins√®re dans ce probl√®me.

### Environnement de travail
De plus, les gens ont l‚Äôhabitude de tenir pour acquis que peu importe
l‚Äôenvironnement ou on fait un calcul math√©matique, on va obtenir le m√™me
r√©sultat. En effet, si on calcul 4+6 dans Windows, sur mac os ou sur une
calculatrice, le r√©sultat donnera toujours 10. Or, les ordinateurs ne font pas
vraiment des calculs, ce qui change tout. Ils font des approximations avec un
nombre fini de chiffres apr√®s la virgule. Parfois, quand les ordinateurs font
des calculs complexes, il y a plusieurs fa√ßons de les impl√©menter. En
pratique, quand on regarde n‚Äôimporte quelle m√©thode un peu complexe, il
est possible qu‚Äôon ait des diff√©rences entre les logiciels et les ordinateurs. Un
papier a regard√© l‚Äôimpact de la version de Mac OS sur les r√©sultats d‚Äôune
analyse de morphom√©trie faite avec Freesurfer. Il y a diff√©rentes analyses
rapport√©es dans ce papier. Ce qu‚Äôon a sur ce graphique ce sont diff√©rentes
r√©gions g√©n√©r√©es par Freesurfer. Dans ces barres-l√†, on a 3 versions de iOS X
(versions majeures et versions mineures). On voit la diff√©rence entre les
diff√©rentes versions de Mac en utilisant le m√™me logiciel. On voit des
changements de 10% au niveau du cortex enthorinal seulement en fonction
du logiciel du syst√®me d‚Äôexploitation ! Malheureusement, l‚Äôenvironnement de
travail peut impacter la reproductibilit√© des r√©sultats. Ce n‚Äôest pas propre √†
la neuroimagerie mais c‚Äôest particuli√®rement probl√©matique car on a de
grosses chaines de traitement et nos m√©thodes ne sont pas d√©velopp√©es par
des gens qui s‚Äôy connaissent en traitement num√©rique, donc les gens ne sont
pas conscients des variations normales qui peuvent √™tre caus√©es par
l‚Äôutilisation d‚Äôenvironnements variables.

### Tailles d'effet
Une autre erreur qu‚Äôon voit particuli√®rement en neuroimagerie c‚Äôest
d‚Äôinterpr√©ter une diff√©rence significative comme une diff√©rence importante.
Une diff√©rence significative signifierait que les populations √©tudi√©es sont
compl√®tement diff√©rentes. Par exemple, on trouve que les amygdales des
personnes sur le spectre de l‚Äôautisme est plus petite que pour les gens
neurotypiques. Cela signifie que la diff√©rence de la moyenne des
distributions est diff√©rente, pas que tous les individus du groupe des gens
ayant un TSA ont une amygdale plus petite que les gens neurotypiques.
Malheureusement, beaucoup de chercheurs et de professionnels ont
tendance √† faire ce genre de conclusion. Une mani√®re d‚Äôaller voir la taille de
la diff√©rence est en regardant le d de Cohen. Un d de Cohen de 0.1 serait ce
qu‚Äôon va trouver si on remarque une diff√©rence significative entre nos
moyennes de groupe, tel qu‚Äôillustrer avec l‚Äôexemple tes TSA plus t√¥t. Un d de
Cohen de 0.79 d√©crirait un effet de groupe immense, ou le score de tous les
membres d‚Äôun groupe est inf√©rieur au score de tous les membres de l‚Äôautre
groupe. Or, si on a un N tr√®s grand, m√™me avec un d de Cohen de 0.1, on va
avoir un p de 0.000001. Il est important de comprendre que malgr√© la
significativit√© de la diff√©rence de groupe, le p ne nous dit rien quant √†
l‚Äôimportance de cette diff√©rence. Le vocabulaire est tr√®s important a
comprendre, car beaucoup de gens vont dire qu‚Äôil y a une forte diff√©rence en
raison du p, alors que cette valeur ne nous dit rien au sujet de la taille
d‚Äôeffet. En fait, ce qu‚Äôon doit retenir, c‚Äôest que si on score significatif n‚Äôa pas
de taille d‚Äôeffet, elle n‚Äôa pas de raison d‚Äô√™tre r√©pliqu√©. Pour r√©pliquer un effet,
il doit √™tre significatif et fort. En neuroimagerie surtout, on fait beaucoup de
tests statistiques alors on se retrouve souvent avec des p qui sont tr√®s
faibles. La majorit√© des papiers ne rapportent m√™me pas les tailles d‚Äôeffet,
alors au final, personne ne regarde si l‚Äôeffet est fort. Or, cette question est
tr√®s importante car la taille de la diff√©rence a un impact sur la
reproductibilit√©

### M√©thodes incompl√®tes
Une autre chose importante √† mentionner sont les m√©thodes incompl√®tes.
Cette image a √©t√© faite par @redpenblackpen, un artiste de McGill. Il publie
des petits dessins sur le monde universitaire. Celui-ci est tr√®s int√©ressant. Le
rez de chauss√© repr√©sente le papier publi√©. On a une maison bien propre et
bien rang√©e. On va lire le papier et tout est carr√© et tout est rigoureux.
Parfois, on a du mat√©riel suppl√©mentaire qui est un document qui est moins
regard√© par les reviewers, contenant des vieilles affaires et du mat√©riel qui a
√©t√© utilis√© pour le projet mais qui n‚Äôest pas essentiel dans le papier principal.
C‚Äôest un peu plus le bazar l√†-dedans on voit parfois que beaucoup plus de
choses ont √©t√© analys√©es que ce qui a √©t√© pr√©sent√© dans le papier. Ce qui est
int√©ressant c‚Äôest le 2e sous-sol, ou on voit le vrai travail avec les tentacules
multicolore et la pagaille. En r√©alit√©, des tonnes de choses ont √©t√© essay√©es,
certaines choses pas termin√©es et d‚Äôautres oui mais oubli√©es. Au final,
l‚Äôauteur lui-m√™me serait incapable lui-m√™me de se souvenir de tout ce qui a
√©t√© essay√© dans le processus du projet. De plus, la complexit√© des √©tapes
d‚Äôanalyse en neuroimageries rend encore plus difficile de se souvenir et de
comprendre ce qui a √©t√© fait. Parfois, le num√©ro du logiciel utilis√© n‚Äôest
m√™me pas rapport√© dans le papier et est enfoui loin dans la 2e cave. Ce que
cela veut dire, c‚Äôest que l‚Äôarticle publi√© nous donne une id√©e tr√®s peu
d√©taill√©e et tr√®s peu compl√®te de ce qui a √©t√© fait. En fait, si les gens ne
partagent pas leur code et peur processus complet, il sera pratiquement
impossible de r√©pliquer les r√©sultats obtenus, justement en raison de la
profondeur de la 2e cave.

## Des solutions

### √âtudes pr√©-enregistr√©es
Solutions permettant de r√©pondre √† la crise de la reproductibilit√©
La premi√®re id√©e, serait de modifier la mani√®re dont on publie les articles. Un
des probl√®mes dans le cercle pr√©sent√© au d√©part, c‚Äôest qu‚Äôil y a un temps
ph√©nom√©nal entre l‚Äô√©laboration des hypoth√®ses et la publication en plus de
ne pas avoir de publication des r√©sultats n√©gatifs. Une mani√®re d‚Äô√©liminer √ßa,
c‚Äôest la publication des hypoth√®ses et des plans d‚Äôanalyse. Cela permet aux
reviewers de critiquer la conception de l‚Äô√©tude avant qu‚Äôelle soit termin√©e,
donc permettrait les modifications √† priori si n√©cessaire. De plus, si les
r√©sultats ne collent pas aux hypoth√®ses, ce ne serait pas grave car l‚Äôarticle
serait d√©j√† accept√©. Il existe des articles pr√©enregistr√©s, ou des registered
recall. C‚Äôest comme un article normal, il manque seulement les r√©sultats et
la discussion. Une fois que les gens se sont mis d‚Äôaccord quant a la m√©thode,
l‚Äôarticle est accept√© peu importe les r√©sultats. Cela ne veut pas dire qu‚Äôon ne
peut pas pr√©senter des nouvelles analyses auxquelles on n‚Äôavait pas pens√©
avant. Celles-ci seront alors pr√©sent√©es comme exploratoires comme il se
devrait, plut√¥t que confirmatoires comme c‚Äôest souvent le cas dans les
articles dont la m√©thode n‚Äôest pas accept√©e et publi√©e d‚Äôavance., ce qu‚Äôon
appelle le HARKing. Ce type d‚Äôapproche ne colle pas avec tout ce qu‚Äôon
souhaiterait publier. Ce ne fonctionne pas vraiment bien avec le travail
m√©thodologique, non plus avec les analyses plus exploratoires. Par contre,
pour les articles qui suivent la d√©marche traditionnelle, cela fonctionnerait
bien et r√©pondrait √† beaucoup des probl√®mes vus au d√©but de ce cours.

### Code
Une autre solution serait d‚Äôapprendre √† coder. Automatiser les analyses
permet de les rendre plus facile pour quiconque de les reproduire. Il peut y
avoir des erreurs dans le code, mais elles peuvent √™tre vues et r√©par√©es
avec des traces. Les analyses qui ne reposent pas sur du code repr√©sente un
obstacle majeur √† la reproductibilit√©.

### Partage de code
Ensuite, partager ce code est un peu anxiog√®ne. Souvent, les gens sont
r√©ticents a rendre public le code utilis√© pour g√©n√©rer un article. Comme c‚Äôest
une partie critique du travail de recherche, √ßa vaut la peine d‚Äôapprendre a le
faire comme il faut et de le partager pour aider √† r√©duire le probl√®me de
reproductibilit√©. Beaucoup de gens qui utilisent Github, une plateforme qui
permet de partager le code et aussi de partager les modifications qui y sont
faites avec le temps. En fait, la principale personne qui b√©n√©ficie de la
publicisation de son code est la personne qui le publie, car si le projet √©volue
dans le temps il y a traces de ce qui a √©t√© fait sur cette plateforme. Le
monstre du 2e sous-sol est alors un peu moins cach√© et un peu moins
inconnu pour vous et pour les personnes qui vous lisent √† la suite de la
publication. De plus en plus, on s‚Äôattend que les scripts d‚Äôanalyse soient
rendus publics lors de la publication des papiers.

### Partage de donn√©es
Une autre solution est de partager les donn√©es. Cela facilite la vie des
laboratoires car 1 an et 2 ans apr√®s avoir publi√© un papier, il est possible
qu‚Äôon ne se souvienne m√™me plus ou se trouvent les donn√©es et quelle
version des donn√©es qui a √©t√© utilis√©e. Le partage de celles-ci rend plus facile
de se relire et de retrouver nos traces. Malheureusement, ce n‚Äôest pas facile
de rendre nos donn√©es publiques. De plus en plus de gens poussent pour
rendre le partage de donn√©es plus commun et plus facile, mais ce n‚Äôest pas
encore fait.
Partager ses donn√©es c‚Äôest comme un spectre qui est repr√©sent√© par ce
graphique. Sur l‚Äôaxe des y on a √† quel point c‚Äôest utilisable et sur l‚Äôaxe des x
c‚Äôest a quel point c‚Äôest beaucoup de donn√©es et √ßa prend du temps a
pr√©parer. En bas, on a les donn√©es ADNI ou HCP, des projets ou les gens
publient leurs donn√©es brutes et les donn√©es pr√©trait√©es. Ensuite, on a
d‚Äôautres personnes qui partagent uniquement leurs donn√©es brutes. Par la
suite, on a des gens qui partagent leurs cartes statistiques. Enfin, les
coordonn√©es des types d‚Äôactivation est quelque chose que beaucoup de
gens partagent dans les articles et qui est tr√®s utile, bien que beaucoup
moins riche que les cartes elles-m√™mes. Le jour ou on arrivera a partager nos
donn√©es syst√©matiquement avec nos articles on va avoir de grandes
am√©liorations au niveau de la reproductibilit√©.

### Partage d'environnement
Ensuite, on a des outils qui permettent de partager notre environnement de
travail. Une initiative qui est bien appr√©ci√©e est neurodebian, qui est une
version de Linux qui vient pr√©installer avec un appstore pour la
neuroimagerie. On peut installer directement les logiciels et les syst√®mes
d‚Äôop√©ration. Ainsi, quelqu‚Äôun qui veut reproduire votre environnement
pourrait le faire. Il y a aussi les containers qui permettent de garder tout
l‚Äôenvironnement de travail sur un seul fichier, cela fonctionne sur Linux et il y
a des mani√®res de le faire fonctionner sur Mac et sur Windows. C‚Äôest
beaucoup utilis√© pour la programmation Web, et la communaut√© scientifique
a commenc√© √† l‚Äôutiliser pour am√©liorer la reproductibilit√©.

### Bonnes pratiques
Certains articles se concentrent sur la formulation de ¬´ guides ¬ª de bonnes
pratiques pour diff√©rentes techniques et m√©thodes de recherche. Cela
permet de voir ce qui est le plus utile pour contrer la crise de reproductibilit√©
en fonction des m√©thodes les plus convoit√©es en neuroscience cognitive.
Une autre chose qui peut √™tre faite est d‚Äô√©tudier la puissance statistique en
√©laborant un projet pour savoir si on doit modifier la taille de notre
√©chantillon. Cela peut √™tre fait avec diff√©rents logiciels et avec le site web
suivant : https://rpsychologist.com/d3/nhst/

### La m√©thode scientifique revisit√©e

### Vers une science g√©n√©ralisable

## Conclusions

## Exercices
