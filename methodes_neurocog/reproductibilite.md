---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
(reproductibilite-controverses-chapitre)=
# Reproductibilit√© et controverses
<table>
  <tr>
    <td align="center">
      <a href="https://github.com/elisabethloranger">
        <img src="https://avatars.githubusercontent.com/u/90270981?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>√âlisabeth Loranger</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
    <td align="center">
      <a href="https://github.com/pbellec">
        <img src="https://avatars.githubusercontent.com/u/1670887?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>Pierre bellec</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
  </tr>
</table>

Durant ce cours, nous avons effectu√© un survol de diverses techniques de neuroimagerie qui ouvrent une fen√™tre fascinante sur la structure et la fonction du cerveau. Par contre, ces techniques sont r√©guli√®rement pr√©sent√©es dans des articles scientifiques qui semblent peu cr√©dibles. Dans cet ultime cours, nous allons discuter des controverses entourant la neuroimagerie, et de fa√ßon plus large, de la crise de reproducibilit√© en sciences.

Les objectifs de ce cours sont les suivants :
- Comprendre la crise de reproductibilit√© en sciences.
- Comprendre certaines pratiques scientifiques douteuses qui participent au manque de reproductibilit√© en neurosciences cognitives.
- Conna√Ætre certains outils qui peuvent am√©liorer la reproductibilit√© en neurosciences cognitives.

## La crise de reproductibilit√©

### Une crise? Quelle crise?
```{figure} ./reproductibilite/significant.png
---
width: 600px
name: significant-fig
---
Cette figure illustre un exemple de processus pouvant amener √† un r√©sultat scientifique controvers√© (ainsi qu'un exemple de probl√®me de comparaisons multiples). Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/882/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
En 2016, un sondage effectu√© aupr√®s de 1576 chercheurs a √©t√© men√© dans le but de voir si, dans la
perception des professionnels dans la recherche, il y avait une crise de
reproductibilit√©, et si oui, laquelle ([Baker, 2016](https://doi.org/10.1038/533452a)).
En tout, 90% des chercheurs ont affirm√© dans ce sondage qu'ils pensaient qu‚Äôil y avait effectivement une crise de reproductibilit√© (52% pour une crise significative et 38% pour une crise mod√©r√©e).

La reproductibilit√©, c‚Äôest quoi? Si nous avions acc√®s
aux donn√©es derri√®re ce papier, serait-on capable de refaire les
analyses et d'arriver aux m√™mes conclusions? Un autre concept proche est la r√©plicabilit√©: en recrutant de nouveaux sujets (nouvel √©chantillon) et en faisant exactement ce que les autres chercheurs avaient fait au niveau des outils utilis√©s et des analyses effectu√©es, est-ce que nous trouverions les m√™mes r√©sultats?
Dans le sondage, 70% des personnes sond√©es rapportaient avoir √©chou√© √† reproduire les r√©sultats d'une autre √©quipe de recherche et plus de 50% d'entre eux rapportaient m√™me avoir √©chou√© √† reproduire leurs propres r√©sultats.

Les personnes sond√©es ont aussi √©valu√© les causes probables de cette crise de
reproductibilit√©. Parmi les raisons les plus fr√©quemment mentionn√©es,
on retrouve la _pression de publier_, la _publication s√©lective_ (les gens ne publient
seulement que ce qui fonctionne bien) ainsi que la _puissance statistique limit√©e_.
Ce chapitre cherchera √† expliquer certaines de ces notions plus en d√©tails en commen√ßant par formaliser le processus de g√©n√©ration de connaissances scientifiques.

### La m√©thode scientifique
```{figure} ./reproductibilite/researchcycle_original.png
---
width: 800px
name: researchcycle-original-fig
---
Cette figure illustre le cycle des d√©couvertes scientifiques selon l'approche de la m√©thode scientifique d√©crite par [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences). Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure adapt√©e par P. Bellec est elle-m√™me disponible sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```
La {numref}`researchcycle-original-fig` pr√©sente une version simplifi√©e de la m√©thode scientifique impliqu√©e dans la d√©couverte de connaissances, inspir√©e par la th√©orie de [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences), telle qu'elle est g√©n√©ralement impl√©ment√©e dans la communaut√© de recherche.
 * On commence avec une consultation des publications ant√©rieures: celles-ci repr√©sentent les connaissances qui ont √©t√© accumul√©es par d‚Äôautres chercheurs avant nous.
 * En lisant cette litt√©rature, les chercheuses/chercheurs peuvent prendre connaissance de ce qui a d√©j√† √©t√© d√©couvert et formuler des hypoth√®ses en lien avec ce que l'on sait d√©j√† sur des choses qu‚Äôon ne connait pas encore.
 * Les chercheuses/chercheurs vont alors construire un devis de recherche: nombre de participants, groupes, tests statistiques, etc. Elles/ils vont aussi faire des pr√©dictions concernant les r√©sultats qu‚Äôelles/ils pensent obtenir.
 * Une fois le devis de recherche √©labor√©, il est maintenant temps de recueillir les donn√©es.
 * Ensuite, on analyse les donn√©es en suivant le protocole qui avait √©t√© √©tabli dans le devis de recherche.
 * Il faut alors interpr√©ter les r√©sultats et notamment les comparer √† nos pr√©dictions pour valider ou invalider nos hypoth√®ses.
 * Les r√©sultats de la recherche sont alors publi√©s pour permettre au reste de la communaut√© de recherche de continuer √† formuler de nouvelles hypoth√®ses.

Comme on utilise des statistiques rigoureuses dans cette approche, on ne g√©n√®re qu'une quantit√© limit√©e de faux positifs, et donc, on fait des d√©couvertes scientifiques sans faire trop d'erreurs. En pratique, cette approche peut √™tre adapt√©e de nombreuses mani√®res en y incluant _des pratiques de recherche douteuses_ qui vont compromettre l'int√©grit√© et la rigueur des conclusions de l'√©tude.

### La m√©thode scientifique: hacked
```{figure} ./reproductibilite/researchcycle_hacked.png
---
width: 800px
name: researchcycle-hacked-fig
---
Cette figure illustre les pratiques douteuses qui peuvent affecter n√©gativement l'int√©grit√© du cycle des d√©couvertes scientifiques. Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure int√®gre aussi une image [shutterstock](https://www.shutterstock.com/image-vector/computer-hacker-laptop-icon-787273936), utilis√©e sous licence shutterstock standard.
```
#### Biais de publication
La publication s√©lective est un des probl√®mes les plus importants identifi√©s dans le sondage vu plus t√¥t. Cela signifie que les r√©sultats d'une √©tude ne sont publi√©s que lorsqu‚Äôils sont positifs, c'est √† dire uniquement s'ils confirment les hypoth√®ses de l'√©quipe de recherche. Si ce type de pratique est syst√©matique dans une communaut√© de recherche, il se peut que plusieurs groupes rapportent un r√©sultat, qui semblera alors robuste, alors qu'en fait, un nombre plus important de groupes de recherche n'ont pas pu r√©pliquer cet effet et ne l'ont donc jamais publi√©. Cela vient d√©former compl√®tement la collection de connaissances accumul√©e par la communaut√© scientifique, collection qui sera elle-m√™me √† l'origine des hypoth√®ses des √©tudes futures.

#### p-hacking
Si nous voyons que nos r√©sultats ne correspondent pas √† nos attentes, il est possible que nous nous demandions si nous avons commis une erreur ou si nous avons bien choisi la technique d'analyse optimale. Nous risquons alors de revisiter la mani√®re
√† laquelle nous analysons les donn√©es jusqu'√† ce que les r√©sultats deviennent significatifs. Ce type d'approche a √©t√© baptis√© _p-hacking_. Le p-hacking peut prendre de nombreuses formes: exclusion arbitraire de "valeurs aberrantes", s√©lection d'un sous-groupe qui montre l'effet attendu, changement des param√®tres de pr√©traitement, etc.

#### HARKing
La derni√®re pratique douteuse est baptis√©e le ¬´ HARKing ¬ª. Le terme HARK est un acronyme originaire de l'anglais pour les termes ¬´ Hypothesis after results are known ¬ª, ou bien "d√©finition des hypoth√®ses apr√®s que les r√©sultats soient connus". On va effectuer de nombreux tests √† partir des donn√©es recueillies et on va formuler a posteriori des hypoth√®ses correspondant aux r√©sultats significatifs dans l'√©chantillon. Ce processus n'est pas n√©cessairement malicieux, mais il peut √©merger d'une volont√© d'interpr√©ter les donn√©es √† tout prix. Cette d√©marche n'est pas n√©cessairement probl√©matique, du moment que les hypoth√®ses sont (correctement) pr√©sent√©es comme √©tant de nature exploratoire, guid√©es par les donn√©es, plut√¥t que comme des hypoth√®ses formul√©es a priori de fa√ßon rigoureuse.

## Reproducibilit√© et neuroimagerie
Nous allons maintenant voir comment la neuroimagerie repr√©sente un domaine particuli√®rement propice au p-hacking, ainsi que d'autres facteurs qui peuvent contribuer au manque de reproductibilit√©. Ces facteurs sont tous li√©s √† la complexit√© des chaines de traitement en neuroimagerie.
 * Tout d'abord, il est possible de faire varier de fa√ßon importante les conclusions d'une √©tude juste en modifiant les choix analytiques que l'on fait concernant la chaine de traitement (ce que l'on appelle les **degr√©s de libert√© en recherche**).
 * Ensuite, il est possible de confondre les effets significatifs et les effets importants (on doit consid√©rer la **taille des effets**).
 * Enfin, √† cause de la complexit√© des m√©thodes utilis√©es, il est souvent difficile, voir impossible, de vraiment comprendre les m√©thodes utilis√©es dans un article seulement sur la base du texte de cet article (**m√©thodes incompl√®tes**).

### Degr√©s de libert√© en recherche
```{figure} ./reproductibilite/multi_analyses_fmri.png
---
width: 800px
name: multi-analyses-fmri-fig
---
Cette figure r√©sume les cartes d'activations IRMf g√©n√©r√©es par 64 √©quipes ind√©pendantes √† partir des m√™mes donn√©es et ayant pour objectif de tester la m√™me hypoth√®se. Les √©quipes ont √©t√© s√©par√©es en trois sous-groupes sur la base de la similarit√© spatiale de leurs cartes d'activation √† l'aide d'un algorithme automatique. Le premier groupe (cluster 1) est le plus gros, avec 50 √©quipes, alors que les deux autres sous-groupes incluent 7 √©quipes chacun. Notez les variations importantes entre les trois sous-groupes. Figure tir√©e de [Botvinik-Nezer et al., 2020](https://doi.org/10.1101/843193) sous licence [CC-BY-NC-ND 4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/).
```
Pour chacune des techniques √©tudi√©es dans ces notes de cours, il est n√©cessaire d'impl√©menter une s√©rie d'√©tapes d'analyse et de choisir certains param√®tres pour chacune de ces √©tapes. Dans la mesure o√π l'on n'a pas de v√©rit√© de terrain √† laquelle se r√©f√©rer en neuroimagerie, il n'existe pas de consensus sur le choix optimal concernant ces param√®tres. De plus, ce choix est probablement d√©pendant de la population d'int√©r√™t et des questions de recherche dans une large mesure. Pour quantifier cette variabilit√©, une √©tude r√©cente a invit√© 70 √©quipes de recherche √† analyser le m√™me jeu de donn√©es par activation en IRMf [Botvinik-Nezer et al., 2020](https://doi.org/10.1101/843193) et √† tester les m√™mes hypoth√®ses. Un premier r√©sultat frappant est que chaque √©quipe a utilis√© une approche unique pour analyser les donn√©es, illustrant ainsi le manque criant de standardisation dans le domaine. Un autre r√©sultat frappant est que, pour une hypoth√®se donn√©e, certaines √©quipes ont produit des cartes tr√®s diff√©rentes (voir {numref}`multi-analyses-fmri-fig`). Bien que certains sous-groupes d'√©quipes aient identifi√© des cartes tr√®s similaires, certains choix ont amen√© √† des diff√©rences importantes. Par ailleurs, m√™me pour les √©quipes g√©n√©rant des cartes similaires, leur interpr√©tation de la carte pour r√©pondre √† l'hypoth√®se variait substantiellement!

```{admonition} Degr√©s de libert√© en recherche et p-hacking
:class: tip
:name: researcher-degrees-freedom-tip

Le nombre de param√®tres qu'un chercheur peut manipuler est appel√© _degr√© de libert√© en recherche_. Comme la neuroimagerie a un tr√®s grand nombre de degr√©s de libert√©, cela augmente le risque de p-hacking.
En effet, il est toujours possible de comparer plusieurs approches pour s√©lectionner la "meilleure", c'est-√†-dire celle qui am√®ne les r√©sultats les plus conformes aux hypoth√®ses de l'√©quipe de recherche.
```

```{admonition} Impact des logiciels d'analyse et de l'environnement virtuel
:class: caution attention
:name: software-warning
Au-del√† des param√®tres utilis√©s dans une analyse, des diff√©rences substantielles peuvent venir du choix du logiciel ou de la version du logiciel utilis√© ([Bowring et al., 2019](https://doi.org/10.1002/hbm.24603)). M√™me des changements mineurs peuvent avoir un impact sur les r√©sultats. Et cela n'est pas limit√© au logiciel de neuroimagerie en tant que tel. Un changement de syst√®me d'op√©ration peut lui aussi cr√©er des diff√©rences, par exemple, dans une analyse de morphom√©trie ([Gronenschild et al., 2012](https://doi.org/10.1371/journal.pone.0038234)).
```

### Tailles d'effet
```{code-cell} ipython 3
:tags: ["hide-input", "remove-output"]
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

def sample_data(d, n_samp=1000):
    samp1 = np.random.normal(loc=0, scale=1, size=n_samp)
    samp2 = np.random.normal(loc=d, scale=1, size=n_samp)
    return samp1, samp2

# On g√©n√®re une s√©rie de nombres al√©atoires
# avec diff√©rentes tailles d'effet
rs = np.random.RandomState(0)
list_d = [0, 0.3, 1, 2]
n_samp = 10000
df1 = pd.DataFrame()
df2 = pd.DataFrame()
for d in list_d:
    samp1, samp2 = sample_data(d, n_samp=n_samp)
    df1[f'{d}'] = samp1
    df2[f'{d}'] = samp2

# On stocke toutes les valeurs avec les param√®tres correspondants
# dans un pandas dataframe
df1 = df1.melt(var_name='d')
df1['group'] = 0
df2 = df2.melt(var_name='d')
df2['group'] = 1
df = df1.append(df2, ignore_index=True)

# On visualise les distributions
import seaborn as sns

sns.set_theme(style='darkgrid')
fig = sns.displot(
    df,     
    x='value',
    col='d',
    hue='group',
    kind='kde',
    fill=True,
)
fig.fig.set_dpi(300)

from myst_nb import glue
glue("effect-size-fig", fig.fig, display=False)
```
```{glue:figure} effect-size-fig
:figwidth: 800px
:name: effect-size-fig
Illustration de deux distributions de groupes suivant une loi normale pour diff√©rentes tailles d'effet mesur√©es avec le _d_ de Cohen. Figure g√©n√©r√©e avec du code python √† l'aide de la librairie [seaborn](https://seaborn.pydata.org/) (cliquer sur + pour voir le code). Cette figure produite par P. Bellec est distribu√©e sous license [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```
Une autre erreur commune en neuroimagerie est d‚Äôinterpr√©ter une diff√©rence significative comme √©tant une diff√©rence importante. Par exemple, imaginons que l'on trouve une diff√©rence significative concernant le volume de l'amygdale entre deux groupes: celui-ci serait r√©duit chez des personnes sur le spectre de l‚Äôautisme par rapport √† des individus neurotypiques. Cela signifierait que la diff√©rence de la moyenne des distributions est diff√©rente, mais il se peut tout √† fait qu'un individu sur le spectre ait une amygdale plus grande qu'un individu neurotypique.

Plut√¥t que de seulement se fier √† la significativit√©, il est important de mesurer la taille de
l'effet, c'est √† dire la diff√©rence qui existe entre les deux populations. On peut par exemple consid√©rer la diff√©rence des moyennes que l'on divise ensuite par l'√©cart type des deux populations - une mesure appel√©e le _d_ de Cohen. Un _d_ de Cohen de 0.1 ou 0.2 est courant pour des diff√©rences de groupes entre populations cliniques. Avec ce type de diff√©rence, les distributions des deux groupes se chevauchent de mani√®re importante. Un _d_ de Cohen de 2 d√©crirait pour sa part un effet de groupe tr√®s important, o√π le score de presque tous les membres d‚Äôun groupe est inf√©rieur au score de tous les membres de l‚Äôautre groupe.

```{admonition} Valeur _p_ et taille d'effet
:class: caution attention
:name: p-value-warning

La valeur _p_ ne nous donne aucune information directe sur la taille de l'effet. Une valeur _p_ peut √™tre tr√®s significative, par exemple _p<0.000001_ simplement parce que l'on compare deux groupes ayant une tr√®s grande taille d'√©chantillon, par exemple N=10000 par groupe. Dans ce cas, m√™me de toutes petites diff√©rences peuvent devenir tr√®s significatives.
```

### M√©thodes incompl√®tes
```{figure} ./reproductibilite/machine_learning.png
---
width: 400px
name: machine-learning-fig
---
Cette figure illustre le processus parfois chaotique de d√©veloppement d'une m√©thode optimale et la difficult√© de communiquer ce processus de mani√®re claire et compl√®te dans la section de m√©thodes d'un article. Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/1838/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
Le manque de d√©tails dans la section "M√©thodes" d'un article peut √™tre un autre obstacle majeur √† la reproduction des r√©sultats. Comme les techniques d'analyse utilis√©es en neuroimagerie sont souvent complexes, il est tr√®s rare d'avoir une description compl√®te des m√©thodes. Il est aussi courant d'omettre les √©tapes qui ont amen√© √† la s√©lection des m√©thodes utilis√©es dans l'article. Le texte d'un article scientifique est g√©n√©ralement √©crit de mani√®re √† raconter une histoire claire. Le mat√©riel suppl√©mentaire de l'article contient parfois (mais pas toujours) plus de d√©tails m√©thodologiques ainsi que des exp√©riences suppl√©mentaires, non essentielles au narratif principal de l'article. Il est tout √† fait possible que d'autres analyses soient omises enti√®rement de l'article et que les membres de l'√©quipe de recherche soient eux m√™me incapables de retracer le processus qui a amen√© √† la s√©lection des analyses finales publi√©es dans l'article.

## Des solutions

### √âtudes pr√©-enregistr√©es
```{code-cell} ipython 3
:tags: ["hide-input", "remove-output"]

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")

# donn√©es de https://psyarxiv.com/3czyt
negative_findings = pd.DataFrame({
    'source':['articles', 'nouvelles √©tudes pr√©-enregistr√©es', 'r√©plications pr√©-enregistr√©es'],
    '%min':[5, 55, 58],
    '%max':[20, 66, 74]
    })

# Initialise la figure
fig, ax = plt.subplots(figsize=(6, 4))

# Estimation maximale
sns.set_color_codes("pastel")
sns.barplot(x="%max", y="source", data=negative_findings,
            label="%max", color="b")

# Estimation minimale
sns.set_color_codes("muted")
sns.barplot(x="%min", y="source", data=negative_findings,
            label="%min", color="b")

# L√©gende
ax.legend(ncol=2, loc="upper right", frameon=True)
ax.set(xlim=(0, 100), ylabel="",
       xlabel="Pourcentage de d√©couvertes n√©gatives dans la litt√©rature")
sns.despine(left=True, bottom=True)
fig.set_dpi(300)

from myst_nb import glue
glue("registered-report-fig", fig, display=False)
```
```{glue:figure} registered-report-fig
:figwidth: 600px
:name: registered-report-fig
Pourcentage "d√©couvertes n√©gatives" dans la litt√©rature, c'est √† dire d'analyses qui ne confirment pas les hypoth√®ses de recherche. On compare des articles traditionnels avec des √©tudes pr√©-enregistr√©es pour de nouvelles hypoth√®ses de recherche, et des √©tudes pr√©-enregistr√©es pour des √©tudes de r√©plication de r√©sultats d√©j√† publi√©s. Pour chaque pourcentage, une valeur estim√©e minimale et maximale est fournie. Statistiques tir√©es de [Allen et Mehler, 2018](https://doi.org/10.31234/osf.io/3czyt) sur 127 √©tudes pr√©-enregistr√©es. Figure g√©n√©r√©e avec du code python √† l'aide de la librairie [seaborn](https://seaborn.pydata.org/) (cliquer sur + pour voir le code). Cette figure produite par P. Bellec est distribu√©e sous license [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```

Une premi√®re id√©e qui gagne en popularit√© pour r√©pondre √† la crise de la reproductibilit√© est ce que l'on appelle une **√©tude pr√©-enregistr√©e**. Un des probl√®mes dans le cercle pr√©sent√© en {numref}`research-cycle-original-fig`, c‚Äôest qu‚Äôon choisit de publier que quand on connait les r√©sultats. Comme publier un article est un processus long et co√ªteux (certains journaux demandent plusieurs milliers de dollars de frais de publication) et que les r√©sultats n√©gatifs sont peu valoris√©s, il est compr√©hensible que l'√©quipe de recherche d√©cide simplement de passer au prochain projet plut√¥t qu'investir dans la publication d'un r√©sultat n√©gatif. Une mani√®re d‚Äô√©liminer √ßa,
c‚Äôest de soumetre la publication avec les hypoth√®ses et les plans d‚Äôanalyse, avant de recueillir les donn√©es. Cela permet aux
reviewers de critiquer la conception de l‚Äô√©tude avant qu‚Äôelle soit termin√©e, et permet donc de modifier le protocole de recherche si n√©cessaire. L'article est alors accept√©, _quelque soit le r√©sultat de l'√©tude_. Si les
r√©sultats ne correspondent pas aux hypoth√®ses, l‚Äôarticle
serait d√©j√† accept√© et publi√© tout de m√™me. Cela ne veut pas dire qu‚Äôon ne
peut pas pr√©senter des nouvelles analyses auxquelles on n‚Äôavait pas pens√©
avant. Celles-ci seront alors pr√©sent√©es (correctement) comme exploratoires, plut√¥t que confirmatoires. En d'autres termes, cette approche √©limine le HARKing, et il semble en pratique que cette approche fonctionne (voir {numref}`registered-report-fig`).

### Code
```{figure} ./reproductibilite/python.png
---
width: 800px
name: python-fig
---
Cette figure illustre les avantages d'automatiser les analyses scientifiques √† l'aide de code (de mani√®re m√©taphorique). Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/353/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
Une autre solution pour rendre les analyses scientifiques en neuroimagerie est d'apprendre √† coder. Automatiser les analyses permet de les rendre plus facile pour quiconque de les reproduire. Il peut y avoir des erreurs dans le code, mais elles peuvent √™tre vues et r√©par√©es avec des traces. Les analyses qui ne reposent pas sur du code repr√©sente un
obstacle majeur √† la reproductibilit√©. Pour √™tre vraiment utile, ce code doit √™tre partag√© de mani√®re publique. Ce code constitue alors un artefact de recherche tr√®s important, beaucoup plus d√©taill√© et sp√©cifique que la section de m√©thodes d'un article. Beaucoup de gens utilisent la plateforme [Github](https://github.com/) pour partager le code et aussi de partager les modifications qui y sont faites avec le temps. Il est aussi possible d'archiver une version du code sur une plateforme comme [zenodo](https://zenodo.org/) qui fournit un identifiant unique pour ce code, comme pour un article. Si le code est de haute qualit√© et r√©-utilisable, il est m√™me possible de publier un article sur ce code, dans un journal comme [Journal of Open Source Software](https://joss.theoj.org/). 

### Partage de donn√©es
Une autre solution est de partager les donn√©es. Cela facilite la vie des
laboratoires car 1 an et 2 ans apr√®s avoir publi√© un papier, il est possible
qu‚Äôon ne se souvienne m√™me plus ou se trouvent les donn√©es et quelle
version des donn√©es qui a √©t√© utilis√©e. Le partage de celles-ci rend plus facile
de se relire et de retrouver nos traces. Malheureusement, ce n‚Äôest pas facile
de rendre nos donn√©es publiques. De plus en plus de gens poussent pour
rendre le partage de donn√©es plus commun et plus facile, mais ce n‚Äôest pas
encore fait.
Partager ses donn√©es c‚Äôest comme un spectre qui est repr√©sent√© par ce
graphique. Sur l‚Äôaxe des y on a √† quel point c‚Äôest utilisable et sur l‚Äôaxe des x
c‚Äôest a quel point c‚Äôest beaucoup de donn√©es et √ßa prend du temps a
pr√©parer. En bas, on a les donn√©es ADNI ou HCP, des projets ou les gens
publient leurs donn√©es brutes et les donn√©es pr√©trait√©es. Ensuite, on a
d‚Äôautres personnes qui partagent uniquement leurs donn√©es brutes. Par la
suite, on a des gens qui partagent leurs cartes statistiques. Enfin, les
coordonn√©es des types d‚Äôactivation est quelque chose que beaucoup de
gens partagent dans les articles et qui est tr√®s utile, bien que beaucoup
moins riche que les cartes elles-m√™mes. Le jour ou on arrivera a partager nos
donn√©es syst√©matiquement avec nos articles on va avoir de grandes
am√©liorations au niveau de la reproductibilit√©.

### Partage d'environnement
Ensuite, on a des outils qui permettent de partager notre environnement de
travail. Une initiative qui est bien appr√©ci√©e est neurodebian, qui est une
version de Linux qui vient pr√©installer avec un appstore pour la
neuroimagerie. On peut installer directement les logiciels et les syst√®mes
d‚Äôop√©ration. Ainsi, quelqu‚Äôun qui veut reproduire votre environnement
pourrait le faire. Il y a aussi les containers qui permettent de garder tout
l‚Äôenvironnement de travail sur un seul fichier, cela fonctionne sur Linux et il y
a des mani√®res de le faire fonctionner sur Mac et sur Windows. C‚Äôest
beaucoup utilis√© pour la programmation Web, et la communaut√© scientifique
a commenc√© √† l‚Äôutiliser pour am√©liorer la reproductibilit√©.

### Bonnes pratiques
Certains articles se concentrent sur la formulation de ¬´ guides ¬ª de bonnes
pratiques pour diff√©rentes techniques et m√©thodes de recherche. Cela
permet de voir ce qui est le plus utile pour contrer la crise de reproductibilit√©
en fonction des m√©thodes les plus convoit√©es en neuroscience cognitive.
Une autre chose qui peut √™tre faite est d‚Äô√©tudier la puissance statistique en
√©laborant un projet pour savoir si on doit modifier la taille de notre
√©chantillon. Cela peut √™tre fait avec diff√©rents logiciels et avec le site web
suivant : https://rpsychologist.com/d3/nhst/

### La m√©thode scientifique revisit√©e

### Vers une science g√©n√©ralisable

## Conclusions

## Exercices

```{admonition} Exercice 10.1
:class: note
Choisir la bonne r√©ponse. Pour pouvoir reproduire exactement un r√©sultat de recherche, il est n√©cessaire d‚Äôavoir acc√®s ‚Ä¶
  1. aux donn√©es utilis√©es dans l‚Äô√©tude.
  2. au code utilis√© pour g√©n√©rer les r√©sultats de l‚Äô√©tude, s‚Äôil existe.
  3. √† l‚Äôenvironnement (version des logiciels) utilis√©s dans l‚Äô√©tude.
  4. Toutes ces r√©ponses.
```

```{admonition} Exercice 10.2
:class: note
Vrai/faux. La significativit√© des r√©sultats dans une √©tude de neuroimagerie peut √™tre impact√©e par...
 * Le logiciel que l‚Äôon utilise pour tester l‚Äôhypoth√®se de recherche.
 * Les param√®tres que l‚Äôon choisit pour analyser les donn√©es, par exemple la quantit√© de lissage spatial.
 * Le syst√®me d‚Äôexploitation de l‚Äôordinateur utilis√© pour effectuer les analyses.
 * La version du syst√®me d‚Äôexploitation de l‚Äôordinateur utilis√© pour effectuer les analyses.
```

```{admonition} Exercice 10.3
:class: note
Vrai/faux. La puissance statistique ‚Ä¶
 * Indique la probabilit√© de d√©tecter un effet avec une proc√©dure statistique.
 * contr√¥le le taux de faux positifs.
 * d√©pend du nombre de sujets dans l‚Äô√©tude.
 * d√©pend du seuil de significativit√© choisie pour l‚Äô√©tude (seuil p).
 * d√©pend de la taille de l‚Äôeffet test√©.
```

```{admonition} Exercice 10.4
:class: note
Choisir la bonne r√©ponse. Parmi les proc√©dures suivantes, lesquelles ne sont pas statistiquement valides?
 * Pr√©senter comme hypoth√®se d‚Äôune √©tude une observation, seulement apr√®s que celle-ci soit observ√©e dans les donn√©es.
 * Red√©finir les crit√®res d‚Äôexclusion des participants en ce qui concerne la qualit√© des donn√©es, apr√®s avoir effectu√© une premi√®re analyse des donn√©es.
 * Pr√©senter dans une √©tude uniquement les r√©sultats d‚Äôun sous-groupe du devis de recherche original, parce ce que ce sous-groupe est le seul qui pr√©sente des r√©sultats significatifs.
 * Aucune des proc√©dures a-c n‚Äôest valide.
```

```{admonition} Exercice 10.5
:class: note
Une √©quipe de recherche a effectu√© une √©tude par activation en imagerie optique chez des nouveaux n√©s. Le comit√© d‚Äô√©thique n‚Äôa pas permis de partager les donn√©es de recherche. Proposer deux actions concr√®tes pour am√©liorer malgr√© tout la reproductibilit√© de l‚Äô√©tude.
```

```{admonition} Exercice 10.6
:class: note
Une √©quipe de recherche compare le volume de diff√©rentes r√©gions du cerveau entre deux groupes de sujets (N=20 par groupe), des sujets en sant√© et des sujets pr√©sentant des signes de d√©pression. Pour cela, l‚Äô√©quipe effectue une analyse par volum√©trie automatis√©e, avec un atlas comprenant 90 r√©gions, et teste l‚Äôeffet de groupe sur chaque r√©gion ind√©pendamment avec un mod√®le de r√©gression, qui inclut l‚Äô√¢ge et le sexe des participants. Le niveau de significativit√© est fix√© √† p<0.05. Le seul test significatif est identifi√© au niveau de l‚Äôamygdale (p=0.041). La conclusion de l‚Äô√©tude est ‚ÄúLe volume de l‚Äôamygdale est plus petit chez les individus pr√©sentant des signes de d√©pression, mais le volume de l‚Äôhippocampe est normal‚Äù. Identifier trois probl√®mes majeurs avec cette conclusion.    
```
