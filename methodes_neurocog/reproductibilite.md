---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
(reproductibilite-controverses-chapitre)=
# Reproductibilit√© et controverses
<table>
  <tr>
    <td align="center">
      <a href="https://github.com/elisabethloranger">
        <img src="https://avatars.githubusercontent.com/u/90270981?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>√âlisabeth Loranger</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
    <td align="center">
      <a href="https://github.com/pbellec">
        <img src="https://avatars.githubusercontent.com/u/1670887?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>Pierre bellec</b></sub>
      </a>
      <br />
        <a title="Contenu">ü§î</a>
    </td>
  </tr>
</table>

Durant ce cours, on a pass√© en revue diverses techniques de neuroimagerie qui ouvrent une fen√™tre fascinante sur la structure et la fonction du cerveau. Mais ces techniques sont r√©guli√®rement impliqu√©es dans des articles scientifiques qui semblent peu cr√©dibles. Dans ce cours nous allons discuter des controverses autour de la neuroimagerie, et plus g√©n√©ralement de la crise de reproducibilit√© en sciences.

Les objectifs de ce cours sont les suivants :
- Comprendre la crise de reproductibilit√© en sciences.
- Comprendre certaines pratiques scientifiques douteuses qui participent
au manque de reproductibilit√© en neurosciences cognitives.
- Conna√Ætre certains outils qui peuvent am√©liorer la reproductibilit√© en neurosciences cognitives.

## La crise de reproductibilit√©

### Une crise? Quelle crise?
```{figure} ./reproductibilite/significant.png
---
width: 600px
name: significant-fig
---
Cette figure illustre le processus qui am√®ne √† un r√©sultat scientifique controvers√© (et le probl√®me de comparaisons multiples). Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/882/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
En 2016, un sondage aupr√®s de 1576 chercheurs a √©t√© men√© dans le but de voir si, dans la
perception des professionnels dans la recherche, il y a une crise de
reproductibilit√© et si oui, laquelle ([Baker, 2016](https://doi.org/10.1038/533452a)). En tout, 90% des chercheurs dans ce sondage pensent
qu‚Äôil y a effectivement une crise de reproductibilit√© (52% pour une crise significative et 38% pour une crise mod√©r√©e).

La reproductibilit√©, c‚Äôest quoi ? Si on avait acc√®s
aux donn√©es derri√®re ce papier, est-ce qu‚Äôon serait capable de refaire les
analyses et arriver aux m√™mes conclusions ? Un autre concept proche est la r√©plication: en recrutant de nouveaux sujets et en faisant exactement ce que les autres chercheurs ont fait au niveau des outils utilis√©s et des analyses effectu√©es, est-ce qu'on va trouver les m√™mes r√©sultats ? Dans le sondage, 70% des personnes sond√©es rapportent avoir √©chou√© √† reproduire les r√©sultats d'une autre √©quipe de recherche, et plus de 50% rapportent avoir √©chou√© √† reproduire leurs propres r√©sultats.

Les personnes sond√©es ont aussi √©valu√© les causes probables de cette crise de
reproductibilit√©. Parmi les raisons les plus fr√©quemment mentionn√©es,
on retrouve la _pression √† publier_ et la _publication s√©lective_ (les gens publient
seulement ce qui fonctionne bien) ainsi que la _puissance statistique limit√©e_.
Ce chapitre va expliquer certaines de ces notions plus en d√©tails, en d√©marrant par formaliser le processus de g√©n√©ration de connaissances scientifiques.

### La m√©thode scientifique
```{figure} ./reproductibilite/researchcycle_original.png
---
width: 800px
name: researchcycle-original-fig
---
Cette figure illustre le cycle des d√©couvertes scientifiques, selon l'approche de la m√©thode scientifique d√©crite par [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences). Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure adapt√©e par P. Bellec est elle-m√™me disponible sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```
La figure {numref}`researchcycle-original` pr√©sente une version simplifi√©e de la m√©thode scientifique pour la d√©couverte de connaissances, inspir√©e par la th√©orie de [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences), telle qu'elle est g√©n√©ralement impl√©ment√©e dans la communaut√© de recherche.
 * On commence avec les publications, qui repr√©sentent les connaissances qui ont √©t√© accumul√©es par d‚Äôautres.
 * En lisant cette lit√©rature, les chercheuses/chercheurs peuvent apprendre ce qui a d√©j√† √©t√© d√©couvert, et faire des hypoth√®ses sur des choses qu‚Äôon ne connait pas encore.
 * Les chercheuses/chercheurs vont alors formuler un devis de recherche : nombre de participants, groupes, tests statistiques, etc. Elles/ils vont aussi faire des pr√©dicitions concernant les r√©sultats qu‚Äôelles/ils pensent obtenir.
 * Une fois le devis de recherche √©labor√©, il est temps de recueillir les donn√©es.
 * Ensuite, on analyse les donn√©es en suivant le protocole qui avait √©t√© √©tabli dans le devis de recherche.
 * Il faut alors interpr√©ter les r√©sultats, et notamment les comparer √† nos pr√©dictions pour valider ou invalider nos hypoth√®ses.
 * Les r√©sultats de la recherche sont alors publi√©s pour permettre au reste de la communaut√© de recherche de continuer √† formuler de nouvelles hypoth√®ses.

 Comme on utilise des statistiques rigoureuses dans cette approche, on ne g√©n√®re qu'une quantit√© limit√© de faux positifs, et donc on fait des d√©couvertes scientifiques sans faire trop d'erreurs. En pratique, cette approche peut √™tre adapt√©e de nombreuses mani√®res avec _des pratiques de recherche douteuses_ qui vont compromettre l'int√©grit√© et la rigueur des conclusions de l'√©tude.

### La m√©thode scientifique: hacked
```{figure} ./reproductibilite/researchcycle_hacked.png
---
width: 800px
name: researchcycle-hacked-fig
---
Cette figure illustre les pratiques douteuses qui peuvent affecter n√©gativement l'int√©grit√© du cycle des d√©couvertes scientifiques. Figure adapt√©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure int√®gre aussi une image [shutterstock](https://www.shutterstock.com/image-vector/computer-hacker-laptop-icon-787273936), utilis√©e sous licence shutterstock standard.
```
#### Biais de publication
La publication s√©lective est un de probl√®mes les plus importants identifi√©s dans le sondage vu plus t√¥t. Cela signifie que les r√©sultats d'une √©tude ne sont publi√©s que lorsqu‚Äôils ne sont positifs, c'est √† dire uniquement s'ils confirment les hypoth√®ses de l'√©quipe de recherche. Si ce type de pratique est syst√©matique dans une communaut√© de recherche, il se peut que plusieurs groupes rapporte un r√©sultat, qui semble alors  robuste, alors qu'en fait un nombre plus important de groupes de recherche n'ont pas pu r√©pliquer cet effet, mais sans publier. Cela vient d√©former compl√®tement les connaissances accumul√©es par la communaut√© scientifique, qui est √† l'origine des hypoth√®ses des √©tudes futures.

##### p-hacking
Si on voit que nos r√©sultats ne correspondent pas √† nos attentes, on pourrait se demander si on n'a pas commis une erreur ou peut √™tre qu'on n'a pas choisi la technique d'analyse la plus optimale. On va alors revisiter la mani√®re
dont analyse les donn√©es jusqu'√† ce que les r√©sultats deviennent significatifs. Ce type d'approche a √©t√© baptis√© _p-hacking_. Le p-hacking peut prendre de nombreuses formes: exclusion arbitraire de "valeurs aberrantes", s√©lection d'un sous-groupe qui montre l'effet attendu, changement des param√®tres de pr√©traitements.

#### HARKing
La derni√®re pratique douteuse est baptis√©e le ¬´ HARKing ¬ª. Le terme HARK est un acronyme en anglais pour les termes ¬´ Hypothesis after results are known ¬ª, ou bien "d√©finition des hypoth√®ses apr√®s que les r√©sultats soient connus". On va effectuer de nombreux tests √† partir des donn√©es recueillies, et on formule a posteriori des hypoth√®ses correspondant aux r√©sultats significatifs dans l'√©chantillon. Ce processus n'est pas n√©cessairement malicieux, mais peut √©merger d'une volont√© d'interpr√©ter les donn√©es. Cette d√©marche n'est pas n√©cessairement probl√©matique, du moment que les hypoth√®ses sont (correctement) pr√©sent√©es comme exploratoires, guid√©es par les donn√©es, plut√¥t que comme une hypoth√®se a priori rigoureuse.

## Reproducibilit√© et neuroimagerie
Nous allons maintenant voir comment la neuroimagerie repr√©sente un domaine particuli√®rement propice au p-hacking, et d'autres facteurs qui contribuent au manque de reproductibilit√©. Ces facteurs sont tous li√©s √† la complexit√© des chaines de traitement en neuroimagerie.
 * Tout d'abord, il est possible de faire varier beaucoup les conclusions d'une √©tude juste en modifiant les choix analytiques que l'on fait dans la chaine de traitement (ce que l'on appelle les **degr√©s de libert√© en recherche**).
 * Ensuite, il est possible d'ajuster nos mod√®les de mani√®re excessive √† l'√©chantillon de donn√©es que l'on a acquis, sans que cela g√©n√©ralise √† un jeu de donn√©es ind√©pendant (un m√©canisme appel√© **overfitting**).
 * Enfin, √† cause de la complexit√© des m√©thodes utilis√©es, il est souvent difficile voir impossible de vraiment comprendre les m√©thodes utilis√©es dans un article √† partir du texte de cet article (**m√©thodes incompl√®tes**).

### Degr√©s de libert√© en recherche
```{figure} ./reproductibilite/multi_analyses_fmri.png
---
width: 800px
name: multi-analyses-fmri-fig
---
Cette figure r√©sume les cartes d'activations IRMf g√©n√©r√©es par 64 √©quipes ind√©pendantes, √† partir des m√™mes donn√©es et pour tester la m√™me hypoth√®se. Les √©quipes ont √©t√© s√©par√©es en trois sous groupes, sur la base de la similarit√© spatiale de leurs cartes d'activation √† l'aide d'un algorithme automatique. Le premier groupe (cluster 1) est le plus gros, avec 50 √©quipes, alors que les deux autres sous-groupes incluent 7 √©quipes chaque. Notez les variations importantes entre les trois sous-groupes. Figure tir√©e de [Botvinik-Nezer et al., 2020](https://doi.org/10.1101/843193) sous licence [CC-BY-NC-ND 4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/).
```
Pour chacune des techniques √©tudi√©es dans ces notes de cours, il est n√©cessaire d'impl√©menter une s√©rie d'√©tapes d'analyses, et chaque √©tape demande de choisir certains param√®tres. Dans la mesure o√π l'on n'a pas de v√©rit√© de terrain auquelle se r√©f√©rer en neuroimagerie, il n'existe pas de consensus sur le choix optimal pour ces param√®tres, et ce choix est probablement d√©pendant de la population d'int√©r√™t et des questions de recherche dans une large mesure. Pour quantifier cette variabilit√©, une √©tude r√©cente a invit√© 70 √©quipes de recherche √† analyser le m√™me jeu de donn√©es, et tester les m√™mes hypoth√®ses, sur un jeu de donn√©es par activation en IRMf [Botvinik-Nezer et al., 2020](https://doi.org/10.1101/843193). Un premier r√©sultat frappant est que chaque √©quipe a utilis√© une approche unique pour analyser les donn√©es, illustrant le manque criant de standardisation dans le domaine. Un autre r√©sultat frappant est, pour une hypoth√®se donn√©e, certaines √©quipes ont produit des cartes tr√®s diff√©rentes, voir {numref}`multi-analyses-fmri-fig`. Bien que certains sous-groupes d'√©quipes ont identifi√© des cartes tr√®s similaires, certains choix ont amen√© √† des diff√©rences importantes. Par ailleurs, m√™me pour les √©quipes g√©n√©rant des cartes similaires, leur interpr√©tation de la carte pour r√©pondre √† l'hypoth√®se variait substantiellement!

```{admonition} Degr√©s de libert√© en recherche et p-hacking
:class: tip
:name: researcher-degrees-freedom-tip

Le nombre de param√®tres qu'un chercheur peut manipuler est appel√© _degr√©s de libert√© en recherche_. Comme la neuroimagerie a un tr√®s grand nombre de degr√©s de libert√©, cela augmente le risque de p-hacking, car il est toujours possible de comparer plusieurs approches pour s√©lectionner la "meilleure", c'est √† dire celle qui am√®ne les r√©sultats les plus conformes aux hypoth√®ses de l'√©quipe de recherche.
```

```{admonition} Impact des logiciels d'analyse et de l'environnement
:class: caution attention
:name: softwark-warning
Au del√† des param√®tres utilis√©s dans une analyse, des diff√©rences substantielles peuvent venir du choix du logiciel, ou de la version du logiciel utilis√©e ([Bowring et al., 2019](https://doi.org/10.1002/hbm.24603)). M√™me des changements mineurs peuvent avoir un impact sur les r√©sultats. Et cela n'est pas limit√© au logiciel de neuroimagerie en tant que tel. Un changement de syst√®me d'op√©ration peut lui aussi cr√©er des diff√©rences, par exemple dans une analyse de morphom√©trie ([Gronenschild et al., 2012](https://doi.org/10.1371/journal.pone.0038234)).
```

### Tailles d'effet
Une autre erreur qu‚Äôon voit particuli√®rement en neuroimagerie c‚Äôest
d‚Äôinterpr√©ter une diff√©rence significative comme une diff√©rence importante.
Une diff√©rence significative signifierait que les populations √©tudi√©es sont
compl√®tement diff√©rentes. Par exemple, on trouve que les amygdales des
personnes sur le spectre de l‚Äôautisme est plus petite que pour les gens
neurotypiques. Cela signifie que la diff√©rence de la moyenne des
distributions est diff√©rente, pas que tous les individus du groupe des gens
ayant un TSA ont une amygdale plus petite que les gens neurotypiques.
Malheureusement, beaucoup de chercheurs et de professionnels ont
tendance √† faire ce genre de conclusion. Une mani√®re d‚Äôaller voir la taille de
la diff√©rence est en regardant le d de Cohen. Un d de Cohen de 0.1 serait ce
qu‚Äôon va trouver si on remarque une diff√©rence significative entre nos
moyennes de groupe, tel qu‚Äôillustrer avec l‚Äôexemple tes TSA plus t√¥t. Un d de
Cohen de 0.79 d√©crirait un effet de groupe immense, ou le score de tous les
membres d‚Äôun groupe est inf√©rieur au score de tous les membres de l‚Äôautre
groupe. Or, si on a un N tr√®s grand, m√™me avec un d de Cohen de 0.1, on va
avoir un p de 0.000001. Il est important de comprendre que malgr√© la
significativit√© de la diff√©rence de groupe, le p ne nous dit rien quant √†
l‚Äôimportance de cette diff√©rence. Le vocabulaire est tr√®s important a
comprendre, car beaucoup de gens vont dire qu‚Äôil y a une forte diff√©rence en
raison du p, alors que cette valeur ne nous dit rien au sujet de la taille
d‚Äôeffet. En fait, ce qu‚Äôon doit retenir, c‚Äôest que si on score significatif n‚Äôa pas
de taille d‚Äôeffet, elle n‚Äôa pas de raison d‚Äô√™tre r√©pliqu√©. Pour r√©pliquer un effet,
il doit √™tre significatif et fort. En neuroimagerie surtout, on fait beaucoup de
tests statistiques alors on se retrouve souvent avec des p qui sont tr√®s
faibles. La majorit√© des papiers ne rapportent m√™me pas les tailles d‚Äôeffet,
alors au final, personne ne regarde si l‚Äôeffet est fort. Or, cette question est
tr√®s importante car la taille de la diff√©rence a un impact sur la
reproductibilit√©

### M√©thodes incompl√®tes
```{figure} ./reproductibilite/machine_learning.png
---
width: 600px
name: machine-learning-fig
---
Cette figure illustre le processus parfois chaotique de d√©veloppement d'une m√©thode optimale, et la difficult√© de communiquer ce processus de mani√®re claire et compl√®te dans une section de m√©thodes d'un article. Cette figure est tir√©e de [xkcd webcomic](https://xkcd.com/1838/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
Le manque de d√©tails dans la section "M√©thodes" d'un article peut √™tre nn autre obstacle majeur √† la reproduction des r√©sultats. Comme les techniques d'analyse utilis√©es en neuroimagerie sont souvent complexes, il est tr√®s rare d'avoir une description compl√®te des m√©thodes. Il est aussi courant d'omettre les √©tapes qui ont amen√© √† la s√©lection des m√©thodes utilis√©es dans l'article. Le texte d'un article scientifique est g√©n√©ralement √©crit de mani√®re √† raconter une histoire claire. Le mat√©riel suppl√©mentaire de l'article contient parfois (mais pas toujours) plus de d√©tails m√©thodologiques ainsi que des exp√©riences suppl√©mentaires, non essentielles au narratif principal de l'article. Il se peut tout √† fait que d'autres analyses soient omises enti√®rement de l'article, et que les membres de l'√©quipe de recherche soient eux m√™me incapables de retracer le processus qui a amen√© √† la s√©lection des analyses finales publi√©es dans l'article.

## Des solutions

### √âtudes pr√©-enregistr√©es
Solutions permettant de r√©pondre √† la crise de la reproductibilit√©
La premi√®re id√©e, serait de modifier la mani√®re dont on publie les articles. Un
des probl√®mes dans le cercle pr√©sent√© au d√©part, c‚Äôest qu‚Äôil y a un temps
ph√©nom√©nal entre l‚Äô√©laboration des hypoth√®ses et la publication en plus de
ne pas avoir de publication des r√©sultats n√©gatifs. Une mani√®re d‚Äô√©liminer √ßa,
c‚Äôest la publication des hypoth√®ses et des plans d‚Äôanalyse. Cela permet aux
reviewers de critiquer la conception de l‚Äô√©tude avant qu‚Äôelle soit termin√©e,
donc permettrait les modifications √† priori si n√©cessaire. De plus, si les
r√©sultats ne collent pas aux hypoth√®ses, ce ne serait pas grave car l‚Äôarticle
serait d√©j√† accept√©. Il existe des articles pr√©enregistr√©s, ou des registered
recall. C‚Äôest comme un article normal, il manque seulement les r√©sultats et
la discussion. Une fois que les gens se sont mis d‚Äôaccord quant a la m√©thode,
l‚Äôarticle est accept√© peu importe les r√©sultats. Cela ne veut pas dire qu‚Äôon ne
peut pas pr√©senter des nouvelles analyses auxquelles on n‚Äôavait pas pens√©
avant. Celles-ci seront alors pr√©sent√©es comme exploratoires comme il se
devrait, plut√¥t que confirmatoires comme c‚Äôest souvent le cas dans les
articles dont la m√©thode n‚Äôest pas accept√©e et publi√©e d‚Äôavance., ce qu‚Äôon
appelle le HARKing. Ce type d‚Äôapproche ne colle pas avec tout ce qu‚Äôon
souhaiterait publier. Ce ne fonctionne pas vraiment bien avec le travail
m√©thodologique, non plus avec les analyses plus exploratoires. Par contre,
pour les articles qui suivent la d√©marche traditionnelle, cela fonctionnerait
bien et r√©pondrait √† beaucoup des probl√®mes vus au d√©but de ce cours.

### Code
Une autre solution serait d‚Äôapprendre √† coder. Automatiser les analyses
permet de les rendre plus facile pour quiconque de les reproduire. Il peut y
avoir des erreurs dans le code, mais elles peuvent √™tre vues et r√©par√©es
avec des traces. Les analyses qui ne reposent pas sur du code repr√©sente un
obstacle majeur √† la reproductibilit√©.

### Partage de code
Ensuite, partager ce code est un peu anxiog√®ne. Souvent, les gens sont
r√©ticents a rendre public le code utilis√© pour g√©n√©rer un article. Comme c‚Äôest
une partie critique du travail de recherche, √ßa vaut la peine d‚Äôapprendre a le
faire comme il faut et de le partager pour aider √† r√©duire le probl√®me de
reproductibilit√©. Beaucoup de gens qui utilisent Github, une plateforme qui
permet de partager le code et aussi de partager les modifications qui y sont
faites avec le temps. En fait, la principale personne qui b√©n√©ficie de la
publicisation de son code est la personne qui le publie, car si le projet √©volue
dans le temps il y a traces de ce qui a √©t√© fait sur cette plateforme. Le
monstre du 2e sous-sol est alors un peu moins cach√© et un peu moins
inconnu pour vous et pour les personnes qui vous lisent √† la suite de la
publication. De plus en plus, on s‚Äôattend que les scripts d‚Äôanalyse soient
rendus publics lors de la publication des papiers.

### Partage de donn√©es
Une autre solution est de partager les donn√©es. Cela facilite la vie des
laboratoires car 1 an et 2 ans apr√®s avoir publi√© un papier, il est possible
qu‚Äôon ne se souvienne m√™me plus ou se trouvent les donn√©es et quelle
version des donn√©es qui a √©t√© utilis√©e. Le partage de celles-ci rend plus facile
de se relire et de retrouver nos traces. Malheureusement, ce n‚Äôest pas facile
de rendre nos donn√©es publiques. De plus en plus de gens poussent pour
rendre le partage de donn√©es plus commun et plus facile, mais ce n‚Äôest pas
encore fait.
Partager ses donn√©es c‚Äôest comme un spectre qui est repr√©sent√© par ce
graphique. Sur l‚Äôaxe des y on a √† quel point c‚Äôest utilisable et sur l‚Äôaxe des x
c‚Äôest a quel point c‚Äôest beaucoup de donn√©es et √ßa prend du temps a
pr√©parer. En bas, on a les donn√©es ADNI ou HCP, des projets ou les gens
publient leurs donn√©es brutes et les donn√©es pr√©trait√©es. Ensuite, on a
d‚Äôautres personnes qui partagent uniquement leurs donn√©es brutes. Par la
suite, on a des gens qui partagent leurs cartes statistiques. Enfin, les
coordonn√©es des types d‚Äôactivation est quelque chose que beaucoup de
gens partagent dans les articles et qui est tr√®s utile, bien que beaucoup
moins riche que les cartes elles-m√™mes. Le jour ou on arrivera a partager nos
donn√©es syst√©matiquement avec nos articles on va avoir de grandes
am√©liorations au niveau de la reproductibilit√©.

### Partage d'environnement
Ensuite, on a des outils qui permettent de partager notre environnement de
travail. Une initiative qui est bien appr√©ci√©e est neurodebian, qui est une
version de Linux qui vient pr√©installer avec un appstore pour la
neuroimagerie. On peut installer directement les logiciels et les syst√®mes
d‚Äôop√©ration. Ainsi, quelqu‚Äôun qui veut reproduire votre environnement
pourrait le faire. Il y a aussi les containers qui permettent de garder tout
l‚Äôenvironnement de travail sur un seul fichier, cela fonctionne sur Linux et il y
a des mani√®res de le faire fonctionner sur Mac et sur Windows. C‚Äôest
beaucoup utilis√© pour la programmation Web, et la communaut√© scientifique
a commenc√© √† l‚Äôutiliser pour am√©liorer la reproductibilit√©.

### Bonnes pratiques
Certains articles se concentrent sur la formulation de ¬´ guides ¬ª de bonnes
pratiques pour diff√©rentes techniques et m√©thodes de recherche. Cela
permet de voir ce qui est le plus utile pour contrer la crise de reproductibilit√©
en fonction des m√©thodes les plus convoit√©es en neuroscience cognitive.
Une autre chose qui peut √™tre faite est d‚Äô√©tudier la puissance statistique en
√©laborant un projet pour savoir si on doit modifier la taille de notre
√©chantillon. Cela peut √™tre fait avec diff√©rents logiciels et avec le site web
suivant : https://rpsychologist.com/d3/nhst/

### La m√©thode scientifique revisit√©e

### Vers une science g√©n√©ralisable

## Conclusions

## Exercices
